{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Web Scrapers for Fox News and Vox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html5lib\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fox News Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"}\n",
    "\n",
    "fox_data = []\n",
    "for url in fox_URLs:\n",
    "    fox_r = requests.get(url=url, headers=headers)\n",
    "    #print(fox_r.content[:1000])\n",
    "    fox_soup = BeautifulSoup(fox_r.content, 'html5lib')\n",
    "    table = fox_soup.find('head')\n",
    "    for row in table.find('script', attrs = {'data-n-head':'ssr', 'type':'application/ld+json'}):\n",
    "        to_string = json.loads(row)\n",
    "        fox_article = {}\n",
    "        fox_article['headline'] = to_string['headline']\n",
    "        fox_article['text'] = to_string['articleBody']\n",
    "        fox_data.append(fox_article)\n",
    "    print(\"done!\")\n",
    "    time.sleep(10)\n",
    "for article in fox_data:\n",
    "    article['Source'] = 'Fox News'\n",
    "print(fox_data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vox Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_data = []\n",
    "for url in vox_URLs:\n",
    "    vox_r = requests.get(url=url, headers=headers)\n",
    "    #print(vox_r.content[:1000])\n",
    "    vox_soup = BeautifulSoup(vox_r.content, 'html5lib')\n",
    "    vox_table = vox_soup.find('div', attrs = {'class':'duet--article--lede duet--page-layout--standard-article'})\n",
    "    for row in vox_table.find_next('div', attrs = {'class':'duet--article--article-body-component'}, recursive=False):\n",
    "        #print(row)\n",
    "        vox_article = {}\n",
    "        vox_article['headline'] = vox_table.find('h1', attrs = {'class':'h74scy1 h74scy0 h74scy6 xkp0cga xkp0cg9'}).text\n",
    "        vox_article['subheading'] = vox_table.find('p', attrs = {'class':'duet--article--dangerously-set-cms-markup h74scyh h74scyg xkp0cga'}).text\n",
    "        vox_article['text'] = [i.text for i in vox_table.find_all_next('p', attrs = {'class':'duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1agbrixi lg8ac51 lg8ac50 xkp0cg1'})]\n",
    "        vox_data.append(vox_article)\n",
    "    print(\"done!\")\n",
    "    time.sleep(10)\n",
    "for article in vox_data:\n",
    "    article['Source'] = 'Vox'\n",
    "print(vox_data[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
